
	//DropoutLayer
	/*
	class DropoutLayer : Layer
	{
		public override void getRange(Range r)
		{
		}
		public override void save(BinaryWriter s)
		{
		}
		public override void save(StreamWriter s)
		{
		}
		public override void load(BinaryReader s)
		{
		}
		public override void load(StreamReader s)
		{
		}
		// An inefficient dropout layer
		// Note this is not most efficient implementation since the layer before
		// computed all these activations and now we're just going to drop them :(
		// same goes for backward pass. Also, if we wanted to be efficient at test time
		// we could equivalently be clever and upscale during train and copy pointers during test
		// todo: make more efficient.
		public float drop_prob;
		public bool[] dropped;
		public DropoutLayer(Def def)
			: base(def)
		{

			Def opt = def;

			// computed
			this.out_sx = opt.in_sx;
			this.out_sy = opt.in_sy;
			this.out_depth = opt.in_depth;
			//this.layer_type = "dropout";
			this.drop_prob = opt.drop_prob.Value;
			this.dropped = new bool[this.out_sx * this.out_sy * this.out_depth];//Util.zeros(this.out_sx*this.out_sy*this.out_depth);
		}

		static Random r = new Random();

		public override Vol forward(Vol V, bool is_training)
		{
			this.in_act = V;
			//if(typeof(is_training)==='undefined') { is_training = false; } // default is prediction mode
			Vol V2 = V.clone();
			int N = V.w.Length;
			if (is_training)
			{
				// do dropout
				for (int i = 0; i < N; i++)
				{
					if (r.NextDouble() < this.drop_prob) { V2.w[i] = 0; this.dropped[i] = true; } // drop!
					else { this.dropped[i] = false; }
				}
			}
			else
			{
				// scale the activations during prediction
				for (int i = 0; i < N; i++) { V2.w[i] *= this.drop_prob; }
			}
			this.out_act = V2;
			return this.out_act; // dummy identity function for now
		}
		public override float backward(DataSet y)
		{
			Vol V = this.in_act; // we need to set dw of this
			Vol chain_grad = this.out_act;
			int N = V.w.Length;
			V.dw = Util.zeros(N); // zero out gradient wrt data
			for (int i = 0; i < N; i++)
			{
				if (!(this.dropped[i]))
				{
					V.dw[i] = chain_grad.dw[i]; // copy over the gradient
				}
			}
			return 0;
		}
		public override List<ParamsAndGrads> getParamsAndGrads()
		{
			return new List<ParamsAndGrads>();
		}
		//toJSON: function() {
		//  json.out_depth = this.out_depth;
		//  json.out_sx = this.out_sx;
		//  json.out_sy = this.out_sy;
		//  json.layer_type = this.layer_type;
		//  json.drop_prob = this.drop_prob;
		//  return json;
		//},
		//fromJSON: function(json) {
		//  this.out_depth = json.out_depth;
		//  this.out_sx = json.out_sx;
		//  this.out_sy = json.out_sy;
		//  this.layer_type = json.layer_type; 
		//  this.drop_prob = json.drop_prob;
		//}
	}*/
	//LocalResponseNormalizationLayer
	/*
	class LocalResponseNormalizationLayer : Layer
	{
		public override void getRange(Range r)
		{
		}
		public override void save(BinaryWriter s)
		{
		}
		public override void save(StreamWriter s)
		{
		}
		public override void load(BinaryReader s)
		{
		}
		public override void load(StreamReader s)
		{
		}
		// a bit experimental layer for now. I think it works but I'm not 100%
		// the gradient check is a bit funky. I'll look into this a bit later.
		// Local Response Normalization in window, along depths of volumes
		public float k;
		public float n;
		public float alpha;
		public float beta;

		public Vol S_cache_;

		public LocalResponseNormalizationLayer(Def def)
			: base(def)
		{
			Def opt = def;

			// required
			this.k = opt.k;
			this.n = opt.n;
			this.alpha = opt.alpha;
			this.beta = opt.beta;

			// computed
			this.out_sx = opt.in_sx;
			this.out_sy = opt.in_sy;
			this.out_depth = opt.in_depth;
			//this.layer_type = "lrn";

			// checks
			if (this.n % 2 == 0)
			{
				//console.log('WARNING n should be odd for LRN layer');
				throw new Exception();
			}
		}

		public override Vol forward(Vol V, bool is_training)
		{
			this.in_act = V;

			Vol A = V.cloneAndZero();
			this.S_cache_ = V.cloneAndZero();
			int n2 = (int)Math.Floor(this.n / 2);
			for (int x = 0; x < V.sx; x++)
			{
				for (int y = 0; y < V.sy; y++)
				{
					for (int i = 0; i < V.depth; i++)
					{

						float ai = V.get(x, y, i);

						// normalize in a window of size n
						float den = 0.0f;
						for (int j = Math.Max(0, i - n2); j <= Math.Min(i + n2, V.depth - 1); j++)
						{
							float aa = V.get(x, y, j);
							den += aa * aa;
						}
						den *= this.alpha / this.n;
						den += this.k;
						this.S_cache_.set(x, y, i, den); // will be useful for backprop
						den = (float)Math.Pow(den, this.beta);
						A.set(x, y, i, ai / den);
					}
				}
			}

			this.out_act = A;
			return this.out_act; // dummy identity function for now
		}
		public override float backward(DataSet del_y)
		{
			// evaluate gradient wrt data
			Vol V = this.in_act; // we need to set dw of this
			V.dw = Util.zeros(V.w.Length); // zero out gradient wrt data
			Vol A = this.out_act; // computed in forward pass 

			int n2 = (int)Math.Floor(this.n / 2);
			for (int x = 0; x < V.sx; x++)
			{
				for (int y = 0; y < V.sy; y++)
				{
					for (int i = 0; i < V.depth; i++)
					{

						float chain_grad = this.out_act.get_grad(x, y, i);
						float S = this.S_cache_.get(x, y, i);
						float SB = (float)Math.Pow(S, this.beta);
						float SB2 = SB * SB;

						// normalize in a window of size n
						for (int j = Math.Max((int)0, i - n2); j <= Math.Min(i + n2, V.depth - 1); j++)
						{
							float aj = V.get(x, y, j);
							float g = -aj * this.beta * (float)Math.Pow(S, this.beta - 1) * this.alpha / this.n * 2 * aj;
							if (j == i) g += SB;
							g /= SB2;
							g *= chain_grad;
							V.add_grad(x, y, j, g);
						}

					}
				}
			}
			return 0;
		}
		public override List<ParamsAndGrads> getParamsAndGrads()
		{
			return new List<ParamsAndGrads>();
		}
		//toJSON: function() {
		//  json.k = this.k;
		//  json.n = this.n;
		//  json.alpha = this.alpha; // normalize by size
		//  json.beta = this.beta;
		//  json.out_sx = this.out_sx; 
		//  json.out_sy = this.out_sy;
		//  json.out_depth = this.out_depth;
		//  json.layer_type = this.layer_type;
		//  return json;
		//},
		//fromJSON: function(json) {
		//  this.k = json.k;
		//  this.n = json.n;
		//  this.alpha = json.alpha; // normalize by size
		//  this.beta = json.beta;
		//  this.out_sx = json.out_sx; 
		//  this.out_sy = json.out_sy;
		//  this.out_depth = json.out_depth;
		//  this.layer_type = json.layer_type;
		//}
	}
	*/
	//MaxoutLayer
	/*
	class MaxoutLayer : Layer
	{
		public override void getRange(Range r)
		{
		}
		public override void save(BinaryWriter s)
		{
		}
		public override void save(StreamWriter s)
		{
		}
		public override void load(BinaryReader s)
		{
		}
		public override void load(StreamReader s)
		{
		}
		// Implements Maxout nnonlinearity that computes
		// x -> max(x)
		// where x is a vector of size group_size. Ideally of course,
		// the input size should be exactly divisible by group_size
		int group_size;
		int[] switches;

		public MaxoutLayer(Def def)
			: base(def)
		{
			Def opt = def;

			// required
			this.group_size = opt.group_size != 0 ? opt.group_size : 2;

			// computed
			this.out_sx = opt.in_sx;
			this.out_sy = opt.in_sy;
			this.out_depth = (int)Math.Floor((float)opt.in_depth / this.group_size);
			//this.layer_type = "maxout";

			this.switches = new int[this.out_sx * this.out_sy * this.out_depth];//Util.zeros(this.out_sx*this.out_sy*this.out_depth); // useful for backprop
		}
		public override Vol forward(Vol V, bool is_training)
		{
			this.in_act = V;
			int N = this.out_depth;
			Vol V2 = new Vol(this.out_sx, this.out_sy, this.out_depth, 0.0f);

			// optimization branch. If we're operating on 1D arrays we dont have
			// to worry about keeping track of x,y,d coordinates inside
			// input volumes. In convnets we do :(
			if (this.out_sx == 1 && this.out_sy == 1)
			{
				for (int i = 0; i < N; i++)
				{
					int ix = i * this.group_size; // base index offset
					float a = V.w[ix];
					int ai = 0;
					for (int j = 1; j < this.group_size; j++)
					{
						float a2 = V.w[ix + j];
						if (a2 > a)
						{
							a = a2;
							ai = j;
						}
					}
					V2.w[i] = a;
					this.switches[i] = ix + ai;
				}
			}
			else
			{
				int n = 0; // counter for switches
				for (int x = 0; x < V.sx; x++)
				{
					for (int y = 0; y < V.sy; y++)
					{
						for (int i = 0; i < N; i++)
						{
							int ix = i * this.group_size;
							float a = V.get(x, y, ix);
							int ai = 0;
							for (int j = 1; j < this.group_size; j++)
							{
								float a2 = V.get(x, y, ix + j);
								if (a2 > a)
								{
									a = a2;
									ai = j;
								}
							}
							V2.set(x, y, i, a);
							this.switches[n] = ix + ai;
							n++;
						}
					}
				}

			}
			this.out_act = V2;
			return this.out_act;
		}
		public override float backward(DataSet del)
		{
			Vol V = this.in_act; // we need to set dw of this
			Vol V2 = this.out_act;
			int N = this.out_depth;
			V.dw = Util.zeros(V.w.Length); // zero out gradient wrt data

			// pass the gradient through the appropriate switch
			if (this.out_sx == 1 && this.out_sy == 1)
			{
				for (int i = 0; i < N; i++)
				{
					float chain_grad = V2.dw[i];
					V.dw[this.switches[i]] = chain_grad;
				}
			}
			else
			{
				// bleh okay, lets do this the hard way
				int n = 0; // counter for switches
				for (int x = 0; x < V2.sx; x++)
				{
					for (int y = 0; y < V2.sy; y++)
					{
						for (int i = 0; i < N; i++)
						{
							float chain_grad = V2.get_grad(x, y, i);
							V.set_grad(x, y, this.switches[n], chain_grad);
							n++;
						}
					}
				}
			}
			return 0;
		}
		public override List<ParamsAndGrads> getParamsAndGrads()
		{
			return new List<ParamsAndGrads>();
		}
		//toJSON: function() {
		//  json.out_depth = this.out_depth;
		//  json.out_sx = this.out_sx;
		//  json.out_sy = this.out_sy;
		//  json.layer_type = this.layer_type;
		//  json.group_size = this.group_size;
		//  return json;
		//},
		//fromJSON: function(json) {
		//  this.out_depth = json.out_depth;
		//  this.out_sx = json.out_sx;
		//  this.out_sy = json.out_sy;
		//  this.layer_type = json.layer_type; 
		//  this.group_size = json.group_size;
		//  this.switches = global.zeros(this.group_size);
		//}
	}*/
	//SigmoidLayer
	/*
	class SigmoidLayer : Layer
	{
		public override void getRange(Range r)
		{
		}
		public override void save(BinaryWriter s)
		{
		}
		public override void save(StreamWriter s)
		{
		}
		public override void load(BinaryReader s)
		{
		}
		public override void load(StreamReader s)
		{
		}
		// Implements Sigmoid nnonlinearity elementwise
		// x -> 1/(1+e^(-x))
		// so the output is between 0 and 1.
		public SigmoidLayer(Def def)
			: base(def)
		{
			Def opt = def;

			// computed
			this.out_sx = opt.in_sx;
			this.out_sy = opt.in_sy;
			this.out_depth = opt.in_depth;
			//this.layer_type = "sigmoid";
		}
		public override Vol forward(Vol V, bool is_training)
		{
			this.in_act = V;
			Vol V2 = V.cloneAndZero();
			int N = V.w.Length;
			float[] V2w = V2.w;
			float[] Vw = V.w;
			for (int i = 0; i < N; i++)
			{
				V2w[i] = 1.0f / (1.0f + (float)Math.Exp(-Vw[i]));
			}
			this.out_act = V2;
			return this.out_act;
		}
		public override float backward(DataSet y)
		{
			Vol V = this.in_act; // we need to set dw of this
			Vol V2 = this.out_act;
			int N = V.w.Length;
			V.dw = Util.zeros(N); // zero out gradient wrt data
			for (int i = 0; i < N; i++)
			{
				float v2wi = V2.w[i];
				V.dw[i] = v2wi * (1.0f - v2wi) * V2.dw[i];
			}
			return 0;
		}
		public override List<ParamsAndGrads> getParamsAndGrads()
		{
			return new List<ParamsAndGrads>();
		}
		//toJSON: function() {
		//  json.out_depth = this.out_depth;
		//  json.out_sx = this.out_sx;
		//  json.out_sy = this.out_sy;
		//  json.layer_type = this.layer_type;
		//  return json;
		//},
		//fromJSON: function(json) {
		//  this.out_depth = json.out_depth;
		//  this.out_sx = json.out_sx;
		//  this.out_sy = json.out_sy;
		//  this.layer_type = json.layer_type; 
		//}
	}*/
	//SVMLayer
	/*
	class SVMLayer : Layer
	{
		public override void getRange(Range r)
		{
		}
		public override void save(BinaryWriter s)
		{
		}
		public override void save(StreamWriter s)
		{
		}
		public override void load(BinaryReader s)
		{
		}
		public override void load(StreamReader s)
		{
		}
		public int num_inputs;

		public SVMLayer(Def def)
			: base(def)
		{
			Def opt = def;

			// computed
			this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;
			this.out_depth = this.num_inputs;
			this.out_sx = 1;
			this.out_sy = 1;
			//this.layer_type = "svm";
		}

		public override Vol forward(Vol V, bool is_training)
		{
			this.in_act = V;
			this.out_act = V; // nothing to do, output raw scores
			return V;
		}
		public override float backward(DataSet y)
		{

			// compute and accumulate gradient wrt weights and bias of this layer
			Vol x = this.in_act;
			x.dw = Util.zeros(x.w.Length); // zero out the gradient of input Vol

			float yscore = x.w[y.predict]; // score of ground truth
			float margin = 1.0f;
			float loss = 0.0f;
			for (int i = 0; i < this.out_depth; i++)
			{
				if (-yscore + x.w[i] + margin > 0)
				{
					// violating example, apply loss
					// I love hinge loss, by the way. Truly.
					// Seriously, compare this SVM code with Softmax forward AND backprop code above
					// it's clear which one is superior, not only in code, simplicity
					// and beauty, but also in practice.
					x.dw[i] += 1;
					x.dw[y.predict] -= 1;
					loss += -yscore + x.w[i] + margin;
				}
			}

			return loss;
		}
		public override List<ParamsAndGrads> getParamsAndGrads()
		{
			return new List<ParamsAndGrads>();
		}
		//toJSON: function() {
		//  json.out_depth = this.out_depth;
		//  json.out_sx = this.out_sx;
		//  json.out_sy = this.out_sy;
		//  json.layer_type = this.layer_type;
		//  json.num_inputs = this.num_inputs;
		//  return json;
		//},
		//fromJSON: function(json) {
		//  this.out_depth = json.out_depth;
		//  this.out_sx = json.out_sx;
		//  this.out_sy = json.out_sy;
		//  this.layer_type = json.layer_type;
		//  this.num_inputs = json.num_inputs;
		//}
	}*/



		//LocalResponseNormalizationLayer
	public class LRNLayer : Layer
	{
		// a bit experimental layer for now. I think it works but I'm not 100%
		// the gradient check is a bit funky. I'll look into this a bit later.
		// Local Response Normalization in window, along depths of volumes
		public float k;
		public int n;
		public float alpha;
		public float beta;

		public Vol S_cache_;

		public LRNLayer(float k, int n, float alpha, float beta)
		{

			// required
			this.k = k;
			this.n = n;
			this.alpha = alpha;
			this.beta = beta;

			// computed
			//this.layer_type = "lrn";

			// checks
			//if (this.n % 2 == 0)
			//{
			//	//console.log('WARNING n should be odd for LRN layer');
			//	throw new Exception();
			//}
		}
		public override bool inited()
		{
			return _inited;
		}
		public override void init()
		{
			this.out_sx = in_layer.out_sx;
			this.out_sy = in_layer.out_sy;
			this.out_depth = in_layer.out_depth;
			this.out_act = new Vol(out_sx, out_sy, out_depth, 0);
			this.S_cache_ = new Vol(out_sx, out_sy, out_depth, 0);
			_inited = true;
		}

		public override Vol forward(Instance instance, Vol V)
		{
			this.in_act = V;

			int n2 = this.n / 2;
			for (int x = 0; x < in_act.sx; x++)
			{
				for (int y = 0; y < in_act.sy; y++)
				{
					for (int i = 0; i < in_act.depth; i++)
					{

						float ai = in_act.get(x, y, i);

						// normalize in a window of size n
						float den = 0.0f;
						for (int j = Math.Max(0, i - n2); j <= Math.Min(i + n2, in_act.depth - 1); j++)
						{
							float aa = in_act.get(x, y, j);
							den += aa * aa;
						}
						den *= this.alpha / this.n;
						den += this.k;
						this.S_cache_.set(x, y, i, den); // will be useful for backprop
						den = (float)Math.Pow(den, this.beta);
						out_act.set(x, y, i, ai / den);
					}
				}
			}

			//this.out_act = A;
			return this.out_act; // dummy identity function for now
		}
		public override void backward()
		{
			// evaluate gradient wrt data
			for (int i = 0; i < in_act.dw.size; i++)
			{
				in_act.dw[i] = 0;
			}

			int n2 = this.n / 2;
			for (int x = 0; x < in_act.sx; x++)
			{
				for (int y = 0; y < in_act.sy; y++)
				{
					for (int i = 0; i < in_act.depth; i++)
					{

						float chain_grad = this.out_act.get_grad(x, y, i);
						float S = this.S_cache_.get(x, y, i);
						float SB = (float)Math.Pow(S, this.beta);
						float SB2 = SB * SB;

						// normalize in a window of size n
						for (int j = Math.Max((int)0, i - n2); j <= Math.Min(i + n2, in_act.depth - 1); j++)
						{
							float aj = in_act.get(x, y, j);
							float g = -aj * this.beta * (float)Math.Pow(S, this.beta - 1) * this.alpha / this.n * 2 * aj;
							if (j == i) g += SB;
							g /= SB2;
							g *= chain_grad;
							in_act.add_grad(x, y, j, g);
						}

					}
				}
			}
		}
	}

